{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Analysis of Verbs, Nouns and Adjectives"
      ],
      "metadata": {
        "id": "aWKa7uN2mMp_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"mental/mental-bert-base-uncased\")\n",
        "import torch\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import re\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import os\n",
        "pd.set_option(\"display.max_columns\", None)\n",
        "from scipy.stats import f_oneway\n",
        "import xgboost as xgb\n",
        "import scipy\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_selection import mutual_info_classif\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "\n",
        "#os.chdir('/content/drive/MyDrive/Depression_Classification')\n",
        "\n",
        "\n",
        "def extract_text(input_string):\n",
        "  # Check if the input is a string\n",
        "    if not isinstance(input_string, str):\n",
        "        print(input_string)\n",
        "        raise ValueError(\"Expected a string input\")\n",
        "    # Use regex to find all text within parentheses and remove the text itself\n",
        "    matches = re.findall(r'\\((.*?)\\)', input_string)\n",
        "\n",
        "    if not matches:\n",
        "        return input_string\n",
        "\n",
        "    # Join the extracted texts with a space and return them\n",
        "    return ' '.join(matches)\n",
        "\n",
        "# Below helper function creates question-answer pairs (without filtering)\n",
        "def create_question_answer_pairs(interview):\n",
        "    question_answer_pairs = []\n",
        "    current_question = []\n",
        "    current_response = []\n",
        "\n",
        "    for index, row in interview.iterrows():\n",
        "        row['value'] = extract_text(str(row['value']))\n",
        "        if row['speaker'] == \"Ellie\":\n",
        "            # If there's an existing question and response, store the pair\n",
        "            if current_question and current_response:\n",
        "\n",
        "                question_answer_pairs.append({\n",
        "                    'question': \" \".join(current_question),\n",
        "                    'answer': \". \".join(current_response)\n",
        "                })\n",
        "                current_response = []  # Reset responses for the next question\n",
        "                current_question = []  # Reset question for the next batch\n",
        "\n",
        "            # Add the new question or follow-up from Ellie to the current question\n",
        "            current_question.append(str(row['value']))\n",
        "\n",
        "        elif row['speaker'] == \"Participant\" and current_question:\n",
        "            current_response.append(str(row['value']))\n",
        "\n",
        "    # Add the last question-answer pair if it exists\n",
        "    if current_question and current_response:\n",
        "\n",
        "        question_answer_pairs.append({\n",
        "            'question': \" \".join(current_question),\n",
        "            'answer': \". \".join(current_response)\n",
        "        })\n",
        "\n",
        "    return pd.DataFrame(question_answer_pairs, columns=['question', 'answer'])\n",
        "\n",
        "# Function to create chunks of QA pairs with overlaps\n",
        "def chunk_qa_pairs(df, max_tokens=80, max_overlap_tokens=40):\n",
        "    chunks = []\n",
        "    current_chunk = []\n",
        "    current_chunk_word_count = 0\n",
        "\n",
        "    # Combine questions and answers\n",
        "    qa_pairs = [f\"Interviewer: {row['question']} Interviewee: {row['answer']}\" for _, row in df.iterrows()]\n",
        "\n",
        "    for pair in qa_pairs:\n",
        "        # Count words in the current pair\n",
        "        pair_word_count = len(pair.split())\n",
        "\n",
        "        # Check if adding this pair exceeds the max tokens\n",
        "        if current_chunk_word_count + pair_word_count > max_tokens:\n",
        "            # Save the current chunk\n",
        "            chunks.append(\" \".join(current_chunk))\n",
        "\n",
        "            # Prepare for the next chunk\n",
        "            # Determine overlap (complete QA pairs)\n",
        "            overlap = []\n",
        "            overlap_word_count = 0\n",
        "\n",
        "            # Start from the last added complete pairs until it hits the token limit\n",
        "            for qa in reversed(current_chunk):\n",
        "                overlap_word_count += len(qa.split())\n",
        "                if overlap_word_count >= max_overlap_tokens:\n",
        "                    break\n",
        "                overlap.append(qa)\n",
        "\n",
        "            # Reverse to maintain original order\n",
        "            overlap.reverse()\n",
        "\n",
        "            # Start new chunk with overlap\n",
        "            current_chunk = overlap\n",
        "            current_chunk_word_count = sum(len(q.split()) for q in current_chunk)\n",
        "\n",
        "        # Add the current pair to the chunk\n",
        "        current_chunk.append(pair)\n",
        "        current_chunk_word_count += pair_word_count\n",
        "\n",
        "    # Add the last chunk if it has content\n",
        "    if current_chunk:\n",
        "        chunks.append(\" \".join(current_chunk))\n",
        "\n",
        "    return chunks\n",
        "\n",
        "\n",
        "# Create dictionary of {participant_id : PHQ_Binary}\n",
        "id_depression_label_map = {}\n",
        "all_ids = set()\n",
        "sheet_name = 'Metadata_mapping'\n",
        "## Change File location 1\n",
        "file_path = 'DAIC demographc data.xlsx'\n",
        "data_csv = pd.read_excel(file_path, sheet_name=sheet_name)\n",
        "for i in range(len(data_csv['Participant_ID'])):\n",
        "    id_depression_label_map[data_csv['Participant_ID'][i]] = data_csv['PHQ_Binary'][i]\n",
        "    all_ids.add(data_csv['Participant_ID'][i])\n",
        "\n",
        "\n",
        "sheet_name = 'Interview_Data'\n",
        "#file_path = '/content/drive/MyDrive/Depression_Classification/DAIC demographc data.xlsx'\n",
        "race_csv = pd.read_excel(file_path, sheet_name=sheet_name)\n",
        "first_name = list(race_csv.iloc[0].values)\n",
        "race_csv.drop(index=0, inplace=True)\n",
        "race_csv.columns = ['Participant_ID', 'Condition', 'Age', 'Gender', 'Education', 'Race']\n",
        "\n",
        "id_race_map = {}\n",
        "for i in range(len(race_csv['Participant_ID'])):\n",
        "  id_race_map[list(race_csv['Participant_ID'])[i]] = list(race_csv['Race'])[i] - 1\n",
        "\n",
        "sheet_name = 'Metadata_mapping'\n",
        "#file_path = '/content/drive/MyDrive/Depression_Classification/DAIC demographc data.xlsx'\n",
        "race_csv = pd.read_excel(file_path, sheet_name=sheet_name)\n",
        "first_name = list(race_csv.iloc[0].values)\n",
        "race_csv.drop(index=0, inplace=True)\n",
        "\n",
        "\n",
        "id_gender_map = {}\n",
        "for i in range(len(race_csv['Participant_ID'])):\n",
        "  id_gender_map[list(race_csv['Participant_ID'])[i]] = [0 if list(race_csv['Gender'])[i] == 'male' else 1][0]\n",
        "\n",
        "race = {\n",
        "    0: 'African American',\n",
        "    1: 'Asian',\n",
        "    2: 'White/Caucasian',\n",
        "    3: 'Hispanic',\n",
        "    4: 'Native American',\n",
        "    5: 'Native Hawaiian/Other Pacific Islander',\n",
        "    6: 'Other'\n",
        "}\n",
        "gender = {\n",
        "    0: 'Male',\n",
        "    1: 'Female'\n",
        "}\n",
        "\n",
        "\n",
        "id_depression_label_map = {}\n",
        "all_ids = set()\n",
        "sheet_name = 'Metadata_mapping'\n",
        "#file_path = '/content/drive/MyDrive/Depression_Classification/DAIC demographc data.xlsx'\n",
        "extended_data_csv = pd.read_excel(file_path, sheet_name=sheet_name)\n",
        "for i in range(len(extended_data_csv['Participant_ID'])):\n",
        "    id_depression_label_map[extended_data_csv['Participant_ID'][i]] = extended_data_csv['PHQ_Binary'][i] #[extended_data_csv['PHQ_Binary'][i], extended_data_csv['PHQ_Score'][i]]\n",
        "    all_ids.add(extended_data_csv['Participant_ID'][i])\n",
        "\n",
        "## Change File location 2\n",
        "labels_csv = pd.read_csv('/content/drive/MyDrive/Depression_Classification/train_split_Depression_AVEC2017.csv')\n",
        "for i in range(len(labels_csv['Participant_ID'])):\n",
        "  id_depression_label_map[labels_csv['Participant_ID'][i]] = extended_data_csv['PHQ_Binary'][i]\n",
        "  all_ids.add(labels_csv['Participant_ID'][i])\n",
        "\n",
        "\n",
        "# List of specific Participant_IDs you want to filter\n",
        "participant_ids = list(all_ids)  # Replace with your specific Participant_IDs\n",
        "\n",
        "id_gender_map[302] = 0\n",
        "\n",
        "to_ignore = []\n",
        "## Change File location 3\n",
        "labels_csv = pd.read_csv('/content/drive/MyDrive/Depression_Classification/test_split_Depression_AVEC2017.csv')\n",
        "for i in range(len(labels_csv['participant_ID'])):\n",
        "  to_ignore.append(labels_csv['participant_ID'][i])\n",
        "\n",
        "id_gender_map[0] = []\n",
        "id_gender_map[1] = []\n",
        "for i in id_gender_map:\n",
        "  if id_gender_map[i] == 0:\n",
        "    id_gender_map[0].append(str(i))\n",
        "  if id_gender_map[i] == 1:\n",
        "    id_gender_map[1].append(str(i))\n",
        "male_participant_ids = id_gender_map[0]\n",
        "female_participant_ids = id_gender_map[1]\n",
        "\n",
        "participant_ids = male_participant_ids\n",
        "max_len = 510\n",
        "ids_collected = []\n",
        "all_qas_df = []\n",
        "def collect_train_test_data(directory):\n",
        "    X_train = []\n",
        "    Y_train = []\n",
        "    attention_masks = []\n",
        "    for filename in os.listdir(directory):\n",
        "      if filename.endswith(\".csv\"):\n",
        "            interview_id = re.findall(r'\\d+', filename)[0]\n",
        "          # change here for male and female analysis\n",
        "          #if interview_id in female_participant_ids:\n",
        "            ids_collected.append(interview_id)\n",
        "\n",
        "            file_path = os.path.join(directory, filename)\n",
        "            df = pd.read_csv(file_path, delimiter='\\t')\n",
        "            df.drop(columns=['start_time', 'stop_time'], axis=1, inplace=True)\n",
        "            df.fillna('', inplace=True)\n",
        "\n",
        "            # Step 1: Create QA Pair\n",
        "            qa_df = create_question_answer_pairs(df)\n",
        "            all_qas_df.append(qa_df)\n",
        "            Y_train.append(id_depression_label_map[int(interview_id)])\n",
        "\n",
        "    return all_qas_df, Y_train\n",
        "# Load the data\n",
        "## Change Folder location 1\n",
        "directory = '/content/drive/MyDrive/Depression_Classification/transcripts_csv'\n",
        "all_qas_df, Y_train = collect_train_test_data(directory)\n"
      ],
      "metadata": {
        "id": "Z4XdPU_1mtLG"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WEtBPxAfmBBU",
        "outputId": "685cd14e-bb82-4eb0-d53f-1229b3be8a70"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Nouns in CSV:  8037\n",
            "Verbs in CSV:  5480\n",
            "Adjecties in CSV:  3143\n",
            "Nouns in DF:  3982\n",
            "Verbs in DF:  2501\n",
            "Adjecties in DF:  1572\n",
            "{'Noun_Overlap_Percentage': 25.15864128406122, 'Verb_Overlap_Percentage': 30.164233576642335, 'Adjective_Overlap_Percentage': 34.04390709513204, 'Noun_Overlap_Count': 2022, 'Verb_Overlap_Count': 1653, 'Adjective_Overlap_Count': 1070}\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import spacy\n",
        "\n",
        "# Load the spaCy model for POS tagging\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Function to extract nouns, verbs, and adjectives from a text\n",
        "def extract_pos(text):\n",
        "    doc = nlp(text)\n",
        "    nouns = {token.text.lower() for token in doc if token.pos_ == \"NOUN\"}\n",
        "    verbs = {token.text.lower() for token in doc if token.pos_ == \"VERB\"}\n",
        "    adjectives = {token.text.lower() for token in doc if token.pos_ == \"ADJ\"}\n",
        "    return nouns, verbs, adjectives\n",
        "\n",
        "# Function to analyze unique POS overlap between CSV and dataframes\n",
        "def analyze_pos_overlap(csv_file_path, dataframes_list):\n",
        "    # Load the CSV file\n",
        "    csv_data = pd.read_csv(csv_file_path)\n",
        "    csv_sentences = csv_data[\"clean_text\"]\n",
        "\n",
        "    # Extract unique nouns, verbs, and adjectives from the CSV file\n",
        "    csv_nouns, csv_verbs, csv_adjectives = set(), set(), set()\n",
        "    for sentence in csv_sentences:\n",
        "        nouns, verbs, adjectives = extract_pos(sentence)\n",
        "        csv_nouns.update(nouns)\n",
        "        csv_verbs.update(verbs)\n",
        "        csv_adjectives.update(adjectives)\n",
        "\n",
        "    # Extract unique nouns, verbs, and adjectives from all dataframes combined\n",
        "    df_nouns, df_verbs, df_adjectives = set(), set(), set()\n",
        "    for df in dataframes_list:\n",
        "        combined_sentences = \" \".join(df[\"question\"].tolist() + df[\"answer\"].tolist())\n",
        "        nouns, verbs, adjectives = extract_pos(combined_sentences)\n",
        "        df_nouns.update(nouns)\n",
        "        df_verbs.update(verbs)\n",
        "        df_adjectives.update(adjectives)\n",
        "\n",
        "    # Calculate the intersection (overlap)\n",
        "    noun_overlap = csv_nouns & df_nouns\n",
        "    verb_overlap = csv_verbs & df_verbs\n",
        "    adjective_overlap = csv_adjectives & df_adjectives\n",
        "\n",
        "    # Results as percentages\n",
        "    noun_overlap_percentage = len(noun_overlap) / len(csv_nouns) * 100 if csv_nouns else 0\n",
        "    verb_overlap_percentage = len(verb_overlap) / len(csv_verbs) * 100 if csv_verbs else 0\n",
        "    adjective_overlap_percentage = len(adjective_overlap) / len(csv_adjectives) * 100 if csv_adjectives else 0\n",
        "    print(\"Nouns in CSV: \", len(csv_nouns))\n",
        "    print(\"Verbs in CSV: \", len(csv_verbs))\n",
        "    print(\"Adjecties in CSV: \", len(csv_adjectives))\n",
        "    print(\"Nouns in DF: \", len(df_nouns))\n",
        "    print(\"Verbs in DF: \", len(df_verbs))\n",
        "    print(\"Adjecties in DF: \", len(df_adjectives))\n",
        "    # Return results\n",
        "    return {\n",
        "        \"Noun_Overlap_Percentage\": noun_overlap_percentage,\n",
        "        \"Verb_Overlap_Percentage\": verb_overlap_percentage,\n",
        "        \"Adjective_Overlap_Percentage\": adjective_overlap_percentage,\n",
        "        \"Noun_Overlap_Count\": len(noun_overlap),\n",
        "        \"Verb_Overlap_Count\": len(verb_overlap),\n",
        "        \"Adjective_Overlap_Count\": len(adjective_overlap)\n",
        "    }\n",
        "\n",
        "# Example usage:\n",
        "# Replace with the path to your CSV file and the list of DataFrames\n",
        "\n",
        "## Change File location 4\n",
        "csv_file_path = \"/content/depression_dataset_reddit_cleaned.csv\"\n",
        "dataframes_list = [\n",
        "    pd.DataFrame({\n",
        "        \"question\": [\"What is AI?\", \"Explain deep learning.\"],\n",
        "        \"answer\": [\"AI stands for artificial intelligence.\", \"Deep learning is a subset of machine learning.\"]\n",
        "    })\n",
        "]\n",
        "\n",
        "# Run the analysis\n",
        "results = analyze_pos_overlap(csv_file_path, all_qas_df)\n",
        "\n",
        "# Print the results\n",
        "print(results)\n"
      ]
    }
  ]
}