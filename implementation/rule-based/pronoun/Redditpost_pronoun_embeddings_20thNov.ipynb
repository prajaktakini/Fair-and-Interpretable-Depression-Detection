{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [
    {
     "file_id": "13LpJIN8F-NsDXdxbjxssAZGWFn4Up-VX",
     "timestamp": 1732185599474
    },
    {
     "file_id": "12FjhMWL7-MJ-kJYKoV8AZ7RMugNfrery",
     "timestamp": 1732182243860
    }
   ],
   "gpuType": "T4"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1fJtTU3CHW-I",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1732188372755,
     "user_tz": 420,
     "elapsed": 15644,
     "user": {
      "displayName": "Prajakta Kini",
      "userId": "13555937896832772391"
     }
    },
    "outputId": "14811fef-3d2b-48a4-f4c5-ac8b95b5526f"
   },
   "execution_count": 3,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import os\n",
    "os.chdir('/content/drive/MyDrive/Fairness_NLP/RULE_BASED')\n"
   ],
   "metadata": {
    "id": "iy6g26OZHes7",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1732188373140,
     "user_tz": 420,
     "elapsed": 388,
     "user": {
      "displayName": "Prajakta Kini",
      "userId": "13555937896832772391"
     }
    }
   },
   "execution_count": 4,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "!ls"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lLzPiZ1IHiuf",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1732182671222,
     "user_tz": 420,
     "elapsed": 797,
     "user": {
      "displayName": "Prajakta Kini",
      "userId": "13555937896832772391"
     }
    },
    "outputId": "3422450e-50cd-44f6-ca0e-7ef5d361b1e2"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "depression_dataset_reddit_cleaned.csv  RuleBasedSystem_DepressionAnalysis.ipynb\n",
      "Redditpost_adj_embeddings.ipynb\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import torch\n",
    "print(torch.__version__)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SV5tAbhEIFsw",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1732182676008,
     "user_tz": 420,
     "elapsed": 4788,
     "user": {
      "displayName": "Prajakta Kini",
      "userId": "13555937896832772391"
     }
    },
    "outputId": "12587122-9607-4908-e8a1-5340bd208299"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "2.5.1+cu121\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dyUUL7i_oVOk",
    "outputId": "2c16e893-37ca-4813-df89-4bf4245276ec",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1732190634416,
     "user_tz": 420,
     "elapsed": 2261277,
     "user": {
      "displayName": "Prajakta Kini",
      "userId": "13555937896832772391"
     }
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/spacy/util.py:1740: UserWarning: [W111] Jupyter notebook detected: if using `prefer_gpu()` or `require_gpu()`, include it in the same cell right before `spacy.load()` to ensure that the model is loaded on the correct device. More information: http://spacy.io/usage/v3#jupyter-notebook-gpu\n",
      "  warnings.warn(Warnings.W111)\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at mental/mental-roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Total unique unigrams (pronouns): 128\n",
      "Individual unigram counts:\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import spacy\n",
    "import pickle\n",
    "\n",
    "# Check if GPU is available and set device accordingly\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load spaCy's English model for POS tagging\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Load BERT model and tokenizer, then move model to the selected device\n",
    "#tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "#model = BertModel.from_pretrained('bert-base-uncased').to(device)\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "access_token = \"TOKEN\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"mental/mental-roberta-base\", token=access_token)\n",
    "model = AutoModel.from_pretrained(\"mental/mental-roberta-base\", token=access_token).to(device)\n",
    "# Function to get the embedding of a unigram in the context of a full sentence\n",
    "def get_unigram_embedding_in_context(sentence, unigram):\n",
    "    # Tokenize and prepare input, moving inputs to the GPU if available\n",
    "    inputs = tokenizer(sentence, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "    outputs = model(**inputs)\n",
    "    embeddings = outputs.last_hidden_state.squeeze(0)  # Shape: [seq_length, hidden_size]\n",
    "\n",
    "    # Tokenize unigram separately to locate its position within the sentence\n",
    "    unigram_tokens = tokenizer.tokenize(unigram)\n",
    "    unigram_token_ids = tokenizer.convert_tokens_to_ids(unigram_tokens)\n",
    "\n",
    "    # Locate unigram within the sentence by matching token IDs\n",
    "    sentence_token_ids = inputs['input_ids'].squeeze().tolist()\n",
    "    for i in range(len(sentence_token_ids) - len(unigram_token_ids) + 1):\n",
    "        if sentence_token_ids[i:i + len(unigram_token_ids)] == unigram_token_ids:\n",
    "            # Move embedding to CPU and detach to get the numpy array\n",
    "            unigram_embedding = embeddings[i:i + len(unigram_token_ids)].mean(dim=0).detach().cpu().numpy()\n",
    "            return unigram_embedding\n",
    "\n",
    "    # If unigram is not found, return a zero embedding\n",
    "    return np.zeros(embeddings.size(-1))\n",
    "\n",
    "# Step 1: Read sentences and filter unigrams that are pronouns\n",
    "unigram_embeddings = defaultdict(list)\n",
    "unigram_counts = Counter()\n",
    "\n",
    "# Replace 'your_file.csv' with your CSV file path\n",
    "with open('/content/drive/MyDrive/Fairness_NLP/RULE_BASED/depression_dataset_reddit_cleaned.csv', 'r') as file:\n",
    "    c = 0\n",
    "    reader = csv.reader(file)\n",
    "    for row in reader:\n",
    "        sentence = row[0].strip()\n",
    "\n",
    "        # POS tagging using spaCy to find pronouns\n",
    "        doc = nlp(sentence)\n",
    "        for token in doc:\n",
    "            if token.pos_ in {\"PRON\"}:  # Check if POS is pronoun\n",
    "                unigram = token.text\n",
    "                unigram_counts[unigram] += 1  # Count the occurrence of each unigram\n",
    "                unigram_embedding = get_unigram_embedding_in_context(sentence, unigram)\n",
    "                unigram_embeddings[unigram].append(unigram_embedding)\n",
    "        c += 1\n",
    "\n",
    "\n",
    "# Display total number of unique unigrams and their individual counts\n",
    "print(f\"Total unique unigrams (pronouns): {len(unigram_counts)}\")\n",
    "print(\"Individual unigram counts:\")\n",
    "monitor = {}\n",
    "for unigram, count in unigram_counts.items():\n",
    "    monitor[unigram] = count\n",
    "\n",
    "# Step 2: Calculate average embeddings for unigrams\n",
    "average_unigram_embeddings = {}\n",
    "for unigram, embeddings in unigram_embeddings.items():\n",
    "    if len(embeddings) > 1:\n",
    "        # Average embeddings if unigram appears more than once\n",
    "        avg_embedding = np.mean(embeddings, axis=0)\n",
    "    else:\n",
    "        avg_embedding = embeddings[0]\n",
    "    average_unigram_embeddings[unigram] = avg_embedding\n",
    "\n",
    "# Output results: save to file\n",
    "with open('pronoun_embeddings.pkl', 'wb') as file:\n",
    "    pickle.dump(average_unigram_embeddings, file)\n"
   ]
  }
 ]
}
